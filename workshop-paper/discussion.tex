
\section{Discussion and Limitations}
\label{sec:discussion}

\subsection{Discussion}
\label{sec:observations}
The results in \ref{fig:relativeruntime} are showing that parallelization of the time dimension is has a positive impact on the overall runtime. The biggest impact is introduced when using two and four workers, eight workers minimize the runtime only by a very small amount and in case of Mahout even add additional time. This effect can be explained by the nature of Spark: While jobs can run concurrently, Spark waits until one computation step, in this case task 1 and 2, are completed. Because of reducing the heap usage, the Git history is split in batches of size \emph{k} which are typically ordered chronological. Since the amount of files is mostly increasing over time, the first batches have less computational effort than the ones with new commits. Therefore some workers just do nothing until all jobs are finished. Strengthened by table \ref{tab:variance}, the variance indicates that all job runtimes of task 1 are much more varying than task 2. The second task gets more homogeneous batches since for each refactoring commit one metric can be calculated.\\
The task ratio table \ref{fig:taskratio} represents the way of the repository usage. Assuming that detecting refactorings is more effortful than only calculating some software quality metrics, task 1 is mostly less computational intense than task 2. The difference is how both libraries are accessing files: RefactoringMiner reads file contents directly from the object database, while CK needs a file system as input. Therefore, the repository has to be checked out before each metric calculation. This obviously takes more time than just reading the object database.\\
Finally, the throughput in figure \ref{fig:throughput} shows that even when the effective runtime is decreased by parallelization, there is still some overhead introduced through Spark and data redistribution. This overhead is expressed by the number of less items that can be processed. By using the accumulated time, the waiting time described earlier is not considered.

\subsection{Limitations of the framework}
\textbf{Git locking.} The biggest limitation of the IRIS implementation is the Git repository itself. The locking mechanism forces to clone the repository multiple times when parallel mining is intended. Therefore each worker node is using only one core (and therefore is single threaded) while in most Spark cluster scenarios each worker is capable of multi-threading and has typically more than one core. A local repository replication would allow parallelization on the worker itself but may also impact disk read negatively when accessing the repositories simultaneously. \\
\textbf{Batching.} Batching is needed to avoid out of memory exceptions in the heap. It is also intended to reduce the instantiating frequency of each mining library. Nevertheless, especially task 1 shows that the divide and conquer approach using batches can impact the runtime negatively when one worker finishes the mining process much earlier than the others and parts of the cluster are running while doing nothing.\\
\textbf{Generalization.} Currently the prototype is limited to only two usecases: Mining refactoring operations and calculating the change of software quality metrics. This limits the usefulness and adaptability for other usecases. Hence, the steps are only slightly coupled through the list of commit hashes but there are no definitions about what input and output formats are available. Additionally, the prototype shows the different repository access used by CK and RefactoringMiner. A non-standardized repository access impacts the distribution behavior and overall performance as seen in the fine-grained runtime analysis.\\
\textbf{Spark UDF.} Using a Git repository directly on each worker is in general an anti-pattern when writing Spark jobs. While the mining operations are running within one node of the DAG, it can be seen as a user defined function (UDF). Those functions are more or less black boxes for Spark as it does not know the internal calculations and can not optimize the DAG with techniques like query optimization.\\
\textbf{File dimension parallelization.} Unlike PyDriller, the prototype does not yet make use of parallelization of the file dimension.
\label{sec:limitations}
