
\section{Introduction}
\label{sec:intro}
The success of machine learning heavily relies on the amount and quality of data the algorithm is trained on. This is true not only for analyzing source code repositories but all learning strategies in general. At the same time, the generation and extraction of suitable data is often based on a manual process which is costly and time-consuming. While getting data for other machine learning settings is often difficult, the process of mining software repositories benefits from GitHub as one of the most important data sources and therefore has a high ability to be automated. The availability of GitHub data through big and public available open-source projects is one of the key aspects in MSR. Automation not only decreases the time spent on building datasets, it also improves the reproducibility of research projects and reusability of mining fragments throughout different projects when designed with generalization in mind.

However, in this preliminary work we show that the level of automation can be extended and improved by building a pipeline for automated repository mining and also improve the computation performance by applying a distributed parallelization approach already renowned in the area of big data processing. An exemplary pipeline is built that is able to detect and extract refactoring operations directly from GitHub repositories and calculates software quality metrics before and after the refactoring operation has been conducted. Eventually,  granular performance data is collected and evaluated to affirm the potential of further improvement and work.
