
\section{Introduction}
\label{sec:intro}
The success of machine learning relies heavily on the amount and quality of dataset the algorithm was trained on. This is true not only for analyzing source code repositories but all learning strategies in general. At the the same time, the generation and extraction of suitable data is often based on a manual process which is costly and time-consuming. While getting the data for other machine learning settings is often difficult, the process of mining of software repositories benefits from GitHub as one of the most important data sources and therefore has a high ability to be automated. The availability of GitHub data through big and public available open-source projects is one of the key aspects in MSR. Automation not only descreases the time spent on building datasets, it also improves the reproducibility of research projects and reusability of mining fragments throughout different projects when designed with generalization in mind.\\
However, we show in this preliminary work that the use of this potential can be extended and improved by building a pipeline for automated repository mining and also increase the computation performance by applying a distributed parallelization approach already renowned in the area of big data processing. An exemplary pipeline is built that is able to detect and extract refactoring operations directly from GitHub repositories and calculates software quality metrics before and after the refactoring operation has been conducted. Eventually,  granular performance data is collected and evaluated to affirm the potential of further improvement and work.