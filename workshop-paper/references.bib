@article{Lin2019a,
abstract = {Recent studies have demonstrated that software is natural, that is, its source code is highly repetitive and predictable like human languages. Also, previous studies suggested the existence of a relationship between code quality and its naturalness, presenting empirical evidence showing that buggy code is 'less natural' than non-buggy code. We conjecture that this qualitynaturalness relationship could be exploited to support refactoring activities (e.g., to locate source code areas in need of refactoring). We perform a first step in this direction by analyzing whether refactoring can improve the naturalness of code. We use state-of-The-Art tools to mine a large dataset of refactoring operations performed in open source systems. Then, we investigate the impact of different types of refactoring operations on the naturalness of the impacted code. We found that (i) code refactoring does not necessarily increase the naturalness of the refactored code; and (ii) the impact on the code naturalness strongly depends on the type of refactoring operations.},
author = {Lin, Bin and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
doi = {10.1109/SANER.2019.8667992},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Lin et al. - On the Impact of Refactoring Operations on Code Quality Metrics - 2019.pdf:pdf},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {Naturalness,Open Source Software,Refactoring},
mendeley-groups = {Software Dependability,Software Dependability/Related Work},
pages = {594--598},
title = {{On the Impact of Refactoring Operations on Code Quality Metrics}},
year = {2019}
}
@article{McCabe1976e,
abstract = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program com- plexity .The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the conelation between intuitive complexity and the graph-theoretic complexity .Several properties of the graph- theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program. The issue of using non structured control flow )s also discussed. A characterization of nonstructured control graphs is given and a method of measuring the "structuredness" of a program is developed. The re- lationship between structure and reducibility is illustrated with several examples. The last section of this paper deals with a testing methodology used in conjunction with the complexity measure; a testing strategy is de- fined that dictates that a program can either admit of a certain minimal testing level or the program can be structurally reduced},
author = {McCabe, Thomas J},
doi = {10.1109/TSE.1976.233837},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/McCabe - A Complexity Measure - 1976.pdf:pdf},
isbn = {2004200111},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity,Masterarbeit/2 Grundlagen/1 Software Qualit{\"{a}}t},
number = {4},
pages = {308--320},
pmid = {300},
title = {{A Complexity Measure}},
year = {1976}
}
@article{Chidamber1994b,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Chidamber, Shyam R. and Kemerer, Chris F.},
doi = {10.1109/32.295895},
eprint = {1011.1669},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Chidamber, Kemerer - A Metrics Suite for Object Oriented Design - 1994.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CR categories and subject descriptors,Class,D.2.8[software engineering],General terms:,K.6.3 [management of computing and information sys,complexity,design,management,management F.2.3 [analysis of algorithms and probl,measurement,metrics,metrics; D.2.9 [software engineering],object orientation,performance,software management,tradeoffs among complexity measures},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity},
number = {6},
pages = {476--493},
pmid = {21645860},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@article{Markovtsev2018,
abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for "Big Code" research.},
archivePrefix = {arXiv},
arxivId = {1803.10144},
author = {Markovtsev, Vadim and Long, Waren},
doi = {10.1145/3196398.3196464},
eprint = {1803.10144},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Markovtsev, Long - Public git archive A big code dataset for all - 2018.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {development history!,git,github,open dataset,software repositories,source code},
mendeley-groups = {Software Dependability/GithubMining},
pages = {34--37},
title = {{Public git archive: A big code dataset for all}},
year = {2018}
}
@article{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed. {\textcopyright} 2012 IEEE.},
author = {Gousios, Georgios and Spinellis, Diomidis},
doi = {10.1109/MSR.2012.6224294},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios, Spinellis - GHTorrent Github's data from a firehose - 2012.pdf:pdf},
isbn = {9781467317610},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {GitHub,commits,dataset,events,repository},
mendeley-groups = {Software Dependability/GithubMining},
pages = {12--21},
title = {{GHTorrent: Github's data from a firehose}},
year = {2012}
}
@article{Spadini2018,
abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present Pydriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that Pydriller can achieve the same results with, on average, 50{\%} less LOC and significantly lower complexity.},
author = {Spadini, Davide and Aniche, Maur{\'{i}}cio and Bacchelli, Alberto},
doi = {10.1145/3236024.3264598},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Spadini, Aniche, Bacchelli - PyDriller Python framework for mining software repositories - 2018.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Git,GitPython,Mining Software Repositories,Python},
mendeley-groups = {Software Dependability/GithubMining},
pages = {908--911},
title = {{PyDriller: Python framework for mining software repositories}},
year = {2018}
}
@article{Gousios2015,
author = {Gousios, Georgios},
doi = {10.1109/MSR.2013.6624034},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios - The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite - 2015.pdf:pdf},
mendeley-groups = {Software Dependability/GithubMining},
number = {June},
title = {{The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite}},
year = {2015}
}
@article{Robles2010,
abstract = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories (MSR) (2004-2006) and now Working Conference on MSR (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that MSR authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole MSR literature and some potential solutions to lower the barriers of replicability are finally presented and discussed. {\textcopyright} 2010 IEEE.},
author = {Robles, Gregorio},
doi = {10.1109/MSR.2010.5463348},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Robles - Replicating MSR A study of the potential replicability of papers published in the Mining Software Repositories Proceedings - 20.pdf:pdf},
isbn = {9781424468034},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Mining software repositories,Public datasets,Replication,Tools},
mendeley-groups = {Software Dependability/GithubMining},
pages = {171--180},
publisher = {IEEE},
title = {{Replicating MSR: A study of the potential replicability of papers published in the Mining Software Repositories Proceedings}},
year = {2010}
}
@misc{repodriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {mauricioaniche/repodriller: a tool to support researchers on mining software repositories studies},
url = {https://github.com/mauricioaniche/repodriller},
urldate = {2020-06-08}
}
@misc{jgit,
mendeley-groups = {Software Dependability/GithubMining},
title = {{JGit | The Eclipse Foundation}},
howpublished = {https://www.eclipse.org/jgit/},
urldate = {2020-06-08}
}
@misc{pydriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {{ishepard/pydriller: Python Framework to analyse Git repositories}},
howpublished = {https://github.com/ishepard/pydriller},
urldate = {2020-06-08}
}
@misc{gitpython,
mendeley-groups = {Software Dependability/GithubMining},
title = {{gitpython-developers/GitPython: GitPython is a python library used to interact with Git repositories.}},
howpublished = {https://github.com/gitpython-developers/GitPython},
urldate = {2020-06-08}
}
@misc{srcdissue,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Getting connection refused {\textperiodcentered} Issue {\#}171 {\textperiodcentered} src-d/datasets}},
howpublished = {https://github.com/src-d/datasets/issues/171},
urldate = {2020-06-08}
}

@inproceedings{Tsantalis:ICSE:2018:RefactoringMiner,
author = {Tsantalis, Nikolaos and Mansouri, Matin and Eshkevari, Laleh M. and Mazinanian, Davood and Dig, Danny},
title = {Accurate and Efficient Refactoring Detection in Commit History},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
series = {ICSE '18},
year = {2018},
isbn = {978-1-4503-5638-1},
location = {Gothenburg, Sweden},
pages = {483--494},
numpages = {12},
url = {http://doi.acm.org/10.1145/3180155.3180206},
doi = {10.1145/3180155.3180206},
acmid = {3180206},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Git, Oracle, abstract syntax tree, accuracy, commit, refactoring},
}

@misc{aniche-ck,
  title={Java code metrics calculator (CK)},
  author={Maurício Aniche},
  year={2015},
  note={Available in https://github.com/mauricioaniche/ck/}
}

@article{Gote2019,
abstract = {Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication, from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10180v1},
author = {Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank},
doi = {10.1109/MSR.2019.00070},
eprint = {arXiv:1903.10180v1},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gote, Scholtes, Schweitzer - Git2net - Mining time-stamped co-editing networks from large git repositories - 2019.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Co editing networks,Collaboration network,Data analysis,Empirical software engineering,Git,Network analysis,Open source,Repository mining,Social networks,Temporal networks},
mendeley-groups = {Software Dependability/GithubMining},
number = {i},
pages = {433--444},
title = {{Git2net - Mining time-stamped co-editing networks from large git repositories}},
volume = {2019-May},
year = {2019}
}

@article{mapreduce2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Dean, Ghemawat - MapReduce Simplified data processing on large clusters - 2008.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {107--113},
title = {{MapReduce: Simplified data processing on large clusters}},
volume = {51},
year = {2008}
}

@inproceedings{hadoop,
abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
author = {Shvachko, K and Kuang, H and Radia, S and Chansler, R},
booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
doi = {10.1109/MSST.2010.5496972},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Shvachko et al. - The Hadoop Distributed File System - 2010.pdf:pdf},
keywords = {Bandwidth,Clustering algorithms,Computer architecture,Concurrent computing,Distributed computing,Facebook,File servers,File systems,HDFS,Hadoop,Hadoop distributed file system,Internet,Protection,Protocols,Yahoo!,data storage,data stream,distributed databases,distributed file system,enterprise data,network operating systems},
mendeley-groups = {Masterarbeit/2 Grundlagen/7 Apache Spark},
month = {may},
pages = {1--10},
title = {{The Hadoop Distributed File System}},
year = {2010}
}

@misc{parquet,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Apache Parquet}},
url = {https://parquet.apache.org/},
urldate = {2020-06-13}
}

@article{Lin2019a,
abstract = {Recent studies have demonstrated that software is natural, that is, its source code is highly repetitive and predictable like human languages. Also, previous studies suggested the existence of a relationship between code quality and its naturalness, presenting empirical evidence showing that buggy code is 'less natural' than non-buggy code. We conjecture that this qualitynaturalness relationship could be exploited to support refactoring activities (e.g., to locate source code areas in need of refactoring). We perform a first step in this direction by analyzing whether refactoring can improve the naturalness of code. We use state-of-The-Art tools to mine a large dataset of refactoring operations performed in open source systems. Then, we investigate the impact of different types of refactoring operations on the naturalness of the impacted code. We found that (i) code refactoring does not necessarily increase the naturalness of the refactored code; and (ii) the impact on the code naturalness strongly depends on the type of refactoring operations.},
author = {Lin, Bin and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
doi = {10.1109/SANER.2019.8667992},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Lin et al. - On the Impact of Refactoring Operations on Code Quality Metrics - 2019.pdf:pdf},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {Naturalness,Open Source Software,Refactoring},
mendeley-groups = {Software Dependability,Software Dependability/Related Work},
pages = {594--598},
title = {{On the Impact of Refactoring Operations on Code Quality Metrics}},
year = {2019}
}
@article{McCabe1976e,
abstract = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program com- plexity .The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the conelation between intuitive complexity and the graph-theoretic complexity .Several properties of the graph- theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program. The issue of using non structured control flow )s also discussed. A characterization of nonstructured control graphs is given and a method of measuring the "structuredness" of a program is developed. The re- lationship between structure and reducibility is illustrated with several examples. The last section of this paper deals with a testing methodology used in conjunction with the complexity measure; a testing strategy is de- fined that dictates that a program can either admit of a certain minimal testing level or the program can be structurally reduced},
author = {McCabe, Thomas J},
doi = {10.1109/TSE.1976.233837},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/McCabe - A Complexity Measure - 1976.pdf:pdf},
isbn = {2004200111},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity,Masterarbeit/2 Grundlagen/1 Software Qualit{\"{a}}t},
number = {4},
pages = {308--320},
pmid = {300},
title = {{A Complexity Measure}},
year = {1976}
}
@article{Chidamber1994b,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Chidamber, Shyam R. and Kemerer, Chris F.},
doi = {10.1109/32.295895},
eprint = {1011.1669},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Chidamber, Kemerer - A Metrics Suite for Object Oriented Design - 1994.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CR categories and subject descriptors,Class,D.2.8[software engineering],General terms:,K.6.3 [management of computing and information sys,complexity,design,management,management F.2.3 [analysis of algorithms and probl,measurement,metrics,metrics; D.2.9 [software engineering],object orientation,performance,software management,tradeoffs among complexity measures},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity},
number = {6},
pages = {476--493},
pmid = {21645860},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@article{Markovtsev2018,
abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for "Big Code" research.},
archivePrefix = {arXiv},
arxivId = {1803.10144},
author = {Markovtsev, Vadim and Long, Waren},
doi = {10.1145/3196398.3196464},
eprint = {1803.10144},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Markovtsev, Long - Public git archive A big code dataset for all - 2018.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {development history!,git,github,open dataset,software repositories,source code},
mendeley-groups = {Software Dependability/GithubMining},
pages = {34--37},
title = {{Public git archive: A big code dataset for all}},
year = {2018}
}
@article{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed. {\textcopyright} 2012 IEEE.},
author = {Gousios, Georgios and Spinellis, Diomidis},
doi = {10.1109/MSR.2012.6224294},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios, Spinellis - GHTorrent Github's data from a firehose - 2012.pdf:pdf},
isbn = {9781467317610},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {GitHub,commits,dataset,events,repository},
mendeley-groups = {Software Dependability/GithubMining},
pages = {12--21},
title = {{GHTorrent: Github's data from a firehose}},
year = {2012}
}
@article{Spadini2018,
abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present Pydriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that Pydriller can achieve the same results with, on average, 50{\%} less LOC and significantly lower complexity.},
author = {Spadini, Davide and Aniche, Maur{\'{i}}cio and Bacchelli, Alberto},
doi = {10.1145/3236024.3264598},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Spadini, Aniche, Bacchelli - PyDriller Python framework for mining software repositories - 2018.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Git,GitPython,Mining Software Repositories,Python},
mendeley-groups = {Software Dependability/GithubMining},
pages = {908--911},
title = {{PyDriller: Python framework for mining software repositories}},
year = {2018}
}
@article{Gousios2015,
author = {Gousios, Georgios},
doi = {10.1109/MSR.2013.6624034},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios - The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite - 2015.pdf:pdf},
mendeley-groups = {Software Dependability/GithubMining},
number = {June},
title = {{The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite}},
year = {2015}
}
@article{Robles2010,
abstract = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories (MSR) (2004-2006) and now Working Conference on MSR (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that MSR authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole MSR literature and some potential solutions to lower the barriers of replicability are finally presented and discussed. {\textcopyright} 2010 IEEE.},
author = {Robles, Gregorio},
doi = {10.1109/MSR.2010.5463348},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Robles - Replicating MSR A study of the potential replicability of papers published in the Mining Software Repositories Proceedings - 20.pdf:pdf},
isbn = {9781424468034},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Mining software repositories,Public datasets,Replication,Tools},
mendeley-groups = {Software Dependability/GithubMining},
pages = {171--180},
publisher = {IEEE},
title = {{Replicating MSR: A study of the potential replicability of papers published in the Mining Software Repositories Proceedings}},
year = {2010}
}
@misc{repodriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {mauricioaniche/repodriller: a tool to support researchers on mining software repositories studies},
url = {https://github.com/mauricioaniche/repodriller},
urldate = {2020-06-08}
}
@misc{jgit,
mendeley-groups = {Software Dependability/GithubMining},
title = {{JGit | The Eclipse Foundation}},
url = {https://www.eclipse.org/jgit/},
urldate = {2020-06-08}
}
@misc{pydriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {{ishepard/pydriller: Python Framework to analyse Git repositories}},
url = {https://github.com/ishepard/pydriller},
urldate = {2020-06-08}
}
@misc{gitpython,
mendeley-groups = {Software Dependability/GithubMining},
title = {{gitpython-developers/GitPython: GitPython is a python library used to interact with Git repositories.}},
url = {https://github.com/gitpython-developers/GitPython},
urldate = {2020-06-08}
}
@misc{srcdissue,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Getting connection refused {\textperiodcentered} Issue {\#}171 {\textperiodcentered} src-d/datasets}},
url = {https://github.com/src-d/datasets/issues/171},
urldate = {2020-06-08}
}

@inproceedings{Tsantalis:ICSE:2018:RefactoringMiner,
author = {Tsantalis, Nikolaos and Mansouri, Matin and Eshkevari, Laleh M. and Mazinanian, Davood and Dig, Danny},
title = {Accurate and Efficient Refactoring Detection in Commit History},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
series = {ICSE '18},
year = {2018},
isbn = {978-1-4503-5638-1},
location = {Gothenburg, Sweden},
pages = {483--494},
numpages = {12},
url = {http://doi.acm.org/10.1145/3180155.3180206},
doi = {10.1145/3180155.3180206},
acmid = {3180206},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Git, Oracle, abstract syntax tree, accuracy, commit, refactoring},
}

@article{Gote2019,
abstract = {Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication, from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10180v1},
author = {Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank},
doi = {10.1109/MSR.2019.00070},
eprint = {arXiv:1903.10180v1},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gote, Scholtes, Schweitzer - Git2net - Mining time-stamped co-editing networks from large git repositories - 2019.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Co editing networks,Collaboration network,Data analysis,Empirical software engineering,Git,Network analysis,Open source,Repository mining,Social networks,Temporal networks},
mendeley-groups = {Software Dependability/GithubMining},
number = {i},
pages = {433--444},
title = {{Git2net - Mining time-stamped co-editing networks from large git repositories}},
volume = {2019-May},
year = {2019}
}

@article{mapreduce2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Dean, Ghemawat - MapReduce Simplified data processing on large clusters - 2008.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {107--113},
title = {{MapReduce: Simplified data processing on large clusters}},
volume = {51},
year = {2008}
}

@inproceedings{hadoop,
abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
author = {Shvachko, K and Kuang, H and Radia, S and Chansler, R},
booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
doi = {10.1109/MSST.2010.5496972},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Shvachko et al. - The Hadoop Distributed File System - 2010.pdf:pdf},
keywords = {Bandwidth,Clustering algorithms,Computer architecture,Concurrent computing,Distributed computing,Facebook,File servers,File systems,HDFS,Hadoop,Hadoop distributed file system,Internet,Protection,Protocols,Yahoo!,data storage,data stream,distributed databases,distributed file system,enterprise data,network operating systems},
mendeley-groups = {Masterarbeit/2 Grundlagen/7 Apache Spark},
month = {may},
pages = {1--10},
title = {{The Hadoop Distributed File System}},
year = {2010}
}

@article{lakshman2010cassandra,
author = {Lakshman, Avinash and Malik, Prashant},
title = {Cassandra: A Decentralized Structured Storage System},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/1773912.1773922},
doi = {10.1145/1773912.1773922},
abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
journal = {SIGOPS Oper. Syst. Rev.},
month = apr,
pages = {35–40},
numpages = {6}
}

@misc{graphx,
mendeley-groups = {Software Dependability/GithubMining},
title = {{GraphX - Spark 2.4.6 Documentation}},
howpublished = {https://spark.apache.org/docs/latest/graphx-programming-guide.html},
urldate = {2020-06-15}
}

@article{Arora2019,
abstract = {Understanding dependency relationship between various program elements in an object-oriented system is essential for many software engineering applications. In this paper, we propose a novel approach of transforming a Java project into a connected graph comprising of program elements (represented as graph nodes) connected to each other using ownership and dependency relationships (represented as edges). These graphs, named as JavaRelationshipGraphs (JRG) are created and stored using Neo4j Graph Database. Additionally, the proposed JavaRelationshipGraphs framework provides details about the two-staged conversion process along with the algorithms involved. The JRG framework uses compiled Java project to obtain the corresponding graph, which can be effectively visualized and queried using the Neo4j browser. JRG is capable of representing most of the important object-oriented features like inheritance, encapsulation, method overloading and overriding. Hence, they are suitable for use in software engineering applications like program dependence analysis, code mining, etc.},
author = {Arora, Ritu and Goel, Sanjay},
doi = {10.1145/3305160.3305173},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Arora, Goel - Transforming Java projects into graphs using neo4j graph databases - 2019.pdf:pdf},
isbn = {9781450366427},
journal = {ACM International Conference Proceeding Series},
keywords = {Dependency analysis in Java programs,Dependency graphs,Neo4j graph databases,Object-oriented languages},
pages = {80--84},
title = {{Transforming Java projects into graphs using neo4j graph databases}},
year = {2019}
}

@misc{neo4j,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Neo4j Graph Platform – The Leader in Graph Databases}},
url = {https://neo4j.com/},
howpublished = {https://github.com/neo4j/neo4j},
urldate = {2020-07-11}
}