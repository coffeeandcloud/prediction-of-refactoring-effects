@article{Lin2019a,
abstract = {Recent studies have demonstrated that software is natural, that is, its source code is highly repetitive and predictable like human languages. Also, previous studies suggested the existence of a relationship between code quality and its naturalness, presenting empirical evidence showing that buggy code is 'less natural' than non-buggy code. We conjecture that this qualitynaturalness relationship could be exploited to support refactoring activities (e.g., to locate source code areas in need of refactoring). We perform a first step in this direction by analyzing whether refactoring can improve the naturalness of code. We use state-of-The-Art tools to mine a large dataset of refactoring operations performed in open source systems. Then, we investigate the impact of different types of refactoring operations on the naturalness of the impacted code. We found that (i) code refactoring does not necessarily increase the naturalness of the refactored code; and (ii) the impact on the code naturalness strongly depends on the type of refactoring operations.},
author = {Lin, Bin and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
doi = {10.1109/SANER.2019.8667992},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Lin et al. - On the Impact of Refactoring Operations on Code Quality Metrics - 2019.pdf:pdf},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {Naturalness,Open Source Software,Refactoring},
mendeley-groups = {Software Dependability,Software Dependability/Related Work},
pages = {594--598},
title = {{On the Impact of Refactoring Operations on Code Quality Metrics}},
year = {2019}
}
@article{McCabe1976e,
abstract = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program com- plexity .The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the conelation between intuitive complexity and the graph-theoretic complexity .Several properties of the graph- theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program. The issue of using non structured control flow )s also discussed. A characterization of nonstructured control graphs is given and a method of measuring the "structuredness" of a program is developed. The re- lationship between structure and reducibility is illustrated with several examples. The last section of this paper deals with a testing methodology used in conjunction with the complexity measure; a testing strategy is de- fined that dictates that a program can either admit of a certain minimal testing level or the program can be structurally reduced},
author = {McCabe, Thomas J},
doi = {10.1109/TSE.1976.233837},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/McCabe - A Complexity Measure - 1976.pdf:pdf},
isbn = {2004200111},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity,Masterarbeit/2 Grundlagen/1 Software Qualit{\"{a}}t},
number = {4},
pages = {308--320},
pmid = {300},
title = {{A Complexity Measure}},
year = {1976}
}
@article{Chidamber1994b,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Chidamber, Shyam R. and Kemerer, Chris F.},
doi = {10.1109/32.295895},
eprint = {1011.1669},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Chidamber, Kemerer - A Metrics Suite for Object Oriented Design - 1994.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CR categories and subject descriptors,Class,D.2.8[software engineering],General terms:,K.6.3 [management of computing and information sys,complexity,design,management,management F.2.3 [analysis of algorithms and probl,measurement,metrics,metrics; D.2.9 [software engineering],object orientation,performance,software management,tradeoffs among complexity measures},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity},
number = {6},
pages = {476--493},
pmid = {21645860},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@article{Markovtsev2018,
abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for "Big Code" research.},
archivePrefix = {arXiv},
arxivId = {1803.10144},
author = {Markovtsev, Vadim and Long, Waren},
doi = {10.1145/3196398.3196464},
eprint = {1803.10144},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Markovtsev, Long - Public git archive A big code dataset for all - 2018.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {development history!,git,github,open dataset,software repositories,source code},
mendeley-groups = {Software Dependability/GithubMining},
pages = {34--37},
title = {{Public git archive: A big code dataset for all}},
year = {2018}
}
@article{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed. {\textcopyright} 2012 IEEE.},
author = {Gousios, Georgios and Spinellis, Diomidis},
doi = {10.1109/MSR.2012.6224294},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios, Spinellis - GHTorrent Github's data from a firehose - 2012.pdf:pdf},
isbn = {9781467317610},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {GitHub,commits,dataset,events,repository},
mendeley-groups = {Software Dependability/GithubMining},
pages = {12--21},
title = {{GHTorrent: Github's data from a firehose}},
year = {2012}
}
@article{Spadini2018,
abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present Pydriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that Pydriller can achieve the same results with, on average, 50{\%} less LOC and significantly lower complexity.},
author = {Spadini, Davide and Aniche, Maur{\'{i}}cio and Bacchelli, Alberto},
doi = {10.1145/3236024.3264598},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Spadini, Aniche, Bacchelli - PyDriller Python framework for mining software repositories - 2018.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Git,GitPython,Mining Software Repositories,Python},
mendeley-groups = {Software Dependability/GithubMining},
pages = {908--911},
title = {{PyDriller: Python framework for mining software repositories}},
year = {2018}
}
@article{Gousios2015,
author = {Gousios, Georgios},
doi = {10.1109/MSR.2013.6624034},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios - The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite - 2015.pdf:pdf},
mendeley-groups = {Software Dependability/GithubMining},
number = {June},
title = {{The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite}},
year = {2015}
}
@article{Robles2010,
abstract = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories (MSR) (2004-2006) and now Working Conference on MSR (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that MSR authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole MSR literature and some potential solutions to lower the barriers of replicability are finally presented and discussed. {\textcopyright} 2010 IEEE.},
author = {Robles, Gregorio},
doi = {10.1109/MSR.2010.5463348},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Robles - Replicating MSR A study of the potential replicability of papers published in the Mining Software Repositories Proceedings - 20.pdf:pdf},
isbn = {9781424468034},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Mining software repositories,Public datasets,Replication,Tools},
mendeley-groups = {Software Dependability/GithubMining},
pages = {171--180},
publisher = {IEEE},
title = {{Replicating MSR: A study of the potential replicability of papers published in the Mining Software Repositories Proceedings}},
year = {2010}
}
@misc{repodriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {mauricioaniche/repodriller: a tool to support researchers on mining software repositories studies},
url = {https://github.com/mauricioaniche/repodriller},
urldate = {2020-06-08}
}
@misc{jgit,
mendeley-groups = {Software Dependability/GithubMining},
title = {{JGit | The Eclipse Foundation}},
howpublished = {https://www.eclipse.org/jgit/},
urldate = {2020-06-08}
}
@misc{pydriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {{ishepard/pydriller: Python Framework to analyse Git repositories}},
howpublished = {https://github.com/ishepard/pydriller},
urldate = {2020-06-08}
}
@misc{gitpython,
mendeley-groups = {Software Dependability/GithubMining},
title = {{gitpython-developers/GitPython: GitPython is a python library used to interact with Git repositories.}},
howpublished = {https://github.com/gitpython-developers/GitPython},
urldate = {2020-06-08}
}
@misc{srcdissue,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Getting connection refused {\textperiodcentered} Issue {\#}171 {\textperiodcentered} src-d/datasets}},
howpublished = {https://github.com/src-d/datasets/issues/171},
urldate = {2020-06-08}
}

@inproceedings{Tsantalis:ICSE:2018:RefactoringMiner,
author = {Tsantalis, Nikolaos and Mansouri, Matin and Eshkevari, Laleh M. and Mazinanian, Davood and Dig, Danny},
title = {Accurate and Efficient Refactoring Detection in Commit History},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
series = {ICSE '18},
year = {2018},
isbn = {978-1-4503-5638-1},
location = {Gothenburg, Sweden},
pages = {483--494},
numpages = {12},
url = {http://doi.acm.org/10.1145/3180155.3180206},
doi = {10.1145/3180155.3180206},
acmid = {3180206},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Git, Oracle, abstract syntax tree, accuracy, commit, refactoring},
}

@manual{aniche-ck,
  title={Java code metrics calculator (CK)},
  author={Maurício Aniche},
  year={2015},
  note={Available in https://github.com/mauricioaniche/ck/}
}

@article{Gote2019,
abstract = {Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication, from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10180v1},
author = {Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank},
doi = {10.1109/MSR.2019.00070},
eprint = {arXiv:1903.10180v1},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gote, Scholtes, Schweitzer - Git2net - Mining time-stamped co-editing networks from large git repositories - 2019.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Co editing networks,Collaboration network,Data analysis,Empirical software engineering,Git,Network analysis,Open source,Repository mining,Social networks,Temporal networks},
mendeley-groups = {Software Dependability/GithubMining},
number = {i},
pages = {433--444},
title = {{Git2net - Mining time-stamped co-editing networks from large git repositories}},
volume = {2019-May},
year = {2019}
}

@article{mapreduce2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Dean, Ghemawat - MapReduce Simplified data processing on large clusters - 2008.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {107--113},
title = {{MapReduce: Simplified data processing on large clusters}},
volume = {51},
year = {2008}
}

@inproceedings{hadoop,
abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
author = {Shvachko, K and Kuang, H and Radia, S and Chansler, R},
booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
doi = {10.1109/MSST.2010.5496972},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Shvachko et al. - The Hadoop Distributed File System - 2010.pdf:pdf},
keywords = {Bandwidth,Clustering algorithms,Computer architecture,Concurrent computing,Distributed computing,Facebook,File servers,File systems,HDFS,Hadoop,Hadoop distributed file system,Internet,Protection,Protocols,Yahoo!,data storage,data stream,distributed databases,distributed file system,enterprise data,network operating systems},
mendeley-groups = {Masterarbeit/2 Grundlagen/7 Apache Spark},
month = {may},
pages = {1--10},
title = {{The Hadoop Distributed File System}},
year = {2010}
}

@misc{parquet,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Apache Parquet}},
url = {https://parquet.apache.org/},
urldate = {2020-06-13}
}

@article{Lin2019a,
abstract = {Recent studies have demonstrated that software is natural, that is, its source code is highly repetitive and predictable like human languages. Also, previous studies suggested the existence of a relationship between code quality and its naturalness, presenting empirical evidence showing that buggy code is 'less natural' than non-buggy code. We conjecture that this qualitynaturalness relationship could be exploited to support refactoring activities (e.g., to locate source code areas in need of refactoring). We perform a first step in this direction by analyzing whether refactoring can improve the naturalness of code. We use state-of-The-Art tools to mine a large dataset of refactoring operations performed in open source systems. Then, we investigate the impact of different types of refactoring operations on the naturalness of the impacted code. We found that (i) code refactoring does not necessarily increase the naturalness of the refactored code; and (ii) the impact on the code naturalness strongly depends on the type of refactoring operations.},
author = {Lin, Bin and Nagy, Csaba and Bavota, Gabriele and Lanza, Michele},
doi = {10.1109/SANER.2019.8667992},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Lin et al. - On the Impact of Refactoring Operations on Code Quality Metrics - 2019.pdf:pdf},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {Naturalness,Open Source Software,Refactoring},
mendeley-groups = {Software Dependability,Software Dependability/Related Work},
pages = {594--598},
title = {{On the Impact of Refactoring Operations on Code Quality Metrics}},
year = {2019}
}
@article{McCabe1976e,
abstract = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program com- plexity .The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the conelation between intuitive complexity and the graph-theoretic complexity .Several properties of the graph- theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program. The issue of using non structured control flow )s also discussed. A characterization of nonstructured control graphs is given and a method of measuring the "structuredness" of a program is developed. The re- lationship between structure and reducibility is illustrated with several examples. The last section of this paper deals with a testing methodology used in conjunction with the complexity measure; a testing strategy is de- fined that dictates that a program can either admit of a certain minimal testing level or the program can be structurally reduced},
author = {McCabe, Thomas J},
doi = {10.1109/TSE.1976.233837},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/McCabe - A Complexity Measure - 1976.pdf:pdf},
isbn = {2004200111},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity,Masterarbeit/2 Grundlagen/1 Software Qualit{\"{a}}t},
number = {4},
pages = {308--320},
pmid = {300},
title = {{A Complexity Measure}},
year = {1976}
}
@article{Chidamber1994b,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Chidamber, Shyam R. and Kemerer, Chris F.},
doi = {10.1109/32.295895},
eprint = {1011.1669},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Chidamber, Kemerer - A Metrics Suite for Object Oriented Design - 1994.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {CR categories and subject descriptors,Class,D.2.8[software engineering],General terms:,K.6.3 [management of computing and information sys,complexity,design,management,management F.2.3 [analysis of algorithms and probl,measurement,metrics,metrics; D.2.9 [software engineering],object orientation,performance,software management,tradeoffs among complexity measures},
mendeley-groups = {Bachelorarbeit/Struktur/6 Complexity},
number = {6},
pages = {476--493},
pmid = {21645860},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@article{Markovtsev2018,
abstract = {The number of open source software projects has been growing exponentially. The major online software repository host, GitHub, has accumulated tens of millions of publicly available Git version-controlled repositories. Although the research potential enabled by the available open source code is clearly substantial, no significant large-scale open source code datasets exist. In this paper, we present the Public Git Archive - dataset of 182,014 top-bookmarked Git repositories from GitHub. We describe the novel data retrieval pipeline to reproduce it. We also elaborate on the strategy for performing dataset updates and legal issues. The Public Git Archive occupies 3.0 TB on disk and is an order of magnitude larger than the current source code datasets. The dataset is made available through HTTP and provides the source code of the projects, the related metadata, and development history. The data retrieval pipeline employs an optimized worker queue model and an optimized archive format to efficiently store forked Git repositories, reducing the amount of data to download and persist. Public Git Archive aims to open a myriad of new opportunities for "Big Code" research.},
archivePrefix = {arXiv},
arxivId = {1803.10144},
author = {Markovtsev, Vadim and Long, Waren},
doi = {10.1145/3196398.3196464},
eprint = {1803.10144},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Markovtsev, Long - Public git archive A big code dataset for all - 2018.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {development history!,git,github,open dataset,software repositories,source code},
mendeley-groups = {Software Dependability/GithubMining},
pages = {34--37},
title = {{Public git archive: A big code dataset for all}},
year = {2018}
}
@article{Gousios2012,
abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed. {\textcopyright} 2012 IEEE.},
author = {Gousios, Georgios and Spinellis, Diomidis},
doi = {10.1109/MSR.2012.6224294},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios, Spinellis - GHTorrent Github's data from a firehose - 2012.pdf:pdf},
isbn = {9781467317610},
issn = {21601852},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {GitHub,commits,dataset,events,repository},
mendeley-groups = {Software Dependability/GithubMining},
pages = {12--21},
title = {{GHTorrent: Github's data from a firehose}},
year = {2012}
}
@article{Spadini2018,
abstract = {Software repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present Pydriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that Pydriller can achieve the same results with, on average, 50{\%} less LOC and significantly lower complexity.},
author = {Spadini, Davide and Aniche, Maur{\'{i}}cio and Bacchelli, Alberto},
doi = {10.1145/3236024.3264598},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Spadini, Aniche, Bacchelli - PyDriller Python framework for mining software repositories - 2018.pdf:pdf},
isbn = {9781450355735},
journal = {ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
keywords = {Git,GitPython,Mining Software Repositories,Python},
mendeley-groups = {Software Dependability/GithubMining},
pages = {908--911},
title = {{PyDriller: Python framework for mining software repositories}},
year = {2018}
}
@article{Gousios2015,
author = {Gousios, Georgios},
doi = {10.1109/MSR.2013.6624034},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gousios - The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite - 2015.pdf:pdf},
mendeley-groups = {Software Dependability/GithubMining},
number = {June},
title = {{The GHTorent dataset and tool suite The GHTorent Dataset and Tool Suite}},
year = {2015}
}
@article{Robles2010,
abstract = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories (MSR) (2004-2006) and now Working Conference on MSR (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that MSR authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole MSR literature and some potential solutions to lower the barriers of replicability are finally presented and discussed. {\textcopyright} 2010 IEEE.},
author = {Robles, Gregorio},
doi = {10.1109/MSR.2010.5463348},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Robles - Replicating MSR A study of the potential replicability of papers published in the Mining Software Repositories Proceedings - 20.pdf:pdf},
isbn = {9781424468034},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Mining software repositories,Public datasets,Replication,Tools},
mendeley-groups = {Software Dependability/GithubMining},
pages = {171--180},
publisher = {IEEE},
title = {{Replicating MSR: A study of the potential replicability of papers published in the Mining Software Repositories Proceedings}},
year = {2010}
}
@misc{repodriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {mauricioaniche/repodriller: a tool to support researchers on mining software repositories studies},
url = {https://github.com/mauricioaniche/repodriller},
urldate = {2020-06-08}
}
@misc{jgit,
mendeley-groups = {Software Dependability/GithubMining},
title = {{JGit | The Eclipse Foundation}},
url = {https://www.eclipse.org/jgit/},
urldate = {2020-06-08}
}
@misc{pydriller,
mendeley-groups = {Software Dependability/GithubMining},
title = {{ishepard/pydriller: Python Framework to analyse Git repositories}},
url = {https://github.com/ishepard/pydriller},
urldate = {2020-06-08}
}
@misc{gitpython,
mendeley-groups = {Software Dependability/GithubMining},
title = {{gitpython-developers/GitPython: GitPython is a python library used to interact with Git repositories.}},
url = {https://github.com/gitpython-developers/GitPython},
urldate = {2020-06-08}
}
@misc{srcdissue,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Getting connection refused {\textperiodcentered} Issue {\#}171 {\textperiodcentered} src-d/datasets}},
url = {https://github.com/src-d/datasets/issues/171},
urldate = {2020-06-08}
}

@inproceedings{Tsantalis:ICSE:2018:RefactoringMiner,
author = {Tsantalis, Nikolaos and Mansouri, Matin and Eshkevari, Laleh M. and Mazinanian, Davood and Dig, Danny},
title = {Accurate and Efficient Refactoring Detection in Commit History},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
series = {ICSE '18},
year = {2018},
isbn = {978-1-4503-5638-1},
location = {Gothenburg, Sweden},
pages = {483--494},
numpages = {12},
url = {http://doi.acm.org/10.1145/3180155.3180206},
doi = {10.1145/3180155.3180206},
acmid = {3180206},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Git, Oracle, abstract syntax tree, accuracy, commit, refactoring},
}

@manual{aniche-ck,
  title={Java code metrics calculator (CK)},
  author={Maurício Aniche},
  year={2015},
  note={Available in https://github.com/mauricioaniche/ck/}
}

@article{Gote2019,
abstract = {Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication, from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.},
archivePrefix = {arXiv},
arxivId = {arXiv:1903.10180v1},
author = {Gote, Christoph and Scholtes, Ingo and Schweitzer, Frank},
doi = {10.1109/MSR.2019.00070},
eprint = {arXiv:1903.10180v1},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Gote, Scholtes, Schweitzer - Git2net - Mining time-stamped co-editing networks from large git repositories - 2019.pdf:pdf},
isbn = {9781728134123},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Co editing networks,Collaboration network,Data analysis,Empirical software engineering,Git,Network analysis,Open source,Repository mining,Social networks,Temporal networks},
mendeley-groups = {Software Dependability/GithubMining},
number = {i},
pages = {433--444},
title = {{Git2net - Mining time-stamped co-editing networks from large git repositories}},
volume = {2019-May},
year = {2019}
}

@article{mapreduce2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Dean, Ghemawat - MapReduce Simplified data processing on large clusters - 2008.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {1},
pages = {107--113},
title = {{MapReduce: Simplified data processing on large clusters}},
volume = {51},
year = {2008}
}

@inproceedings{hadoop,
abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
author = {Shvachko, K and Kuang, H and Radia, S and Chansler, R},
booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
doi = {10.1109/MSST.2010.5496972},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Shvachko et al. - The Hadoop Distributed File System - 2010.pdf:pdf},
keywords = {Bandwidth,Clustering algorithms,Computer architecture,Concurrent computing,Distributed computing,Facebook,File servers,File systems,HDFS,Hadoop,Hadoop distributed file system,Internet,Protection,Protocols,Yahoo!,data storage,data stream,distributed databases,distributed file system,enterprise data,network operating systems},
mendeley-groups = {Masterarbeit/2 Grundlagen/7 Apache Spark},
month = {may},
pages = {1--10},
title = {{The Hadoop Distributed File System}},
year = {2010}
}

@misc{parquet,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Apache Parquet}},
url = {https://parquet.apache.org/},
urldate = {2020-06-13}
}

@INPROCEEDINGS{sociotech,  author={M. {Joblin} and W. {Mauerer} and S. {Apel} and J. {Siegmund} and D. {Riehle}},  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},   title={From Developer Networks to Verified Communities: A Fine-Grained Approach},   year={2015},  volume={1},  number={},  pages={563-573},}

@INPROCEEDINGS{palomba,  author={M. {Tufano} and F. {Palomba} and G. {Bavota} and R. {Oliveto} and M. {Di Penta} and A. {De Lucia} and D. {Poshyvanyk}},  booktitle={2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},   title={When and Why Your Code Starts to Smell Bad},   year={2015},  volume={1},  number={},  pages={403-414},}

@article{lakshman2010cassandra,
  title={Cassandra: a decentralized structured storage system},
  author={Lakshman, Avinash and Malik, Prashant},
  journal={ACM SIGOPS Operating Systems Review},
  volume={44},
  number={2},
  pages={35--40},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@misc{graphx,
mendeley-groups = {Software Dependability/GithubMining},
title = {{GraphX - Spark 2.4.6 Documentation}},
howpublished = {https://spark.apache.org/docs/latest/graphx-programming-guide.html},
urldate = {2020-06-15}
}
@inproceedings{miller2013graph,
  title={Graph database applications and concepts with Neo4j},
  author={Miller, Justin J},
  booktitle={Proceedings of the Southern Association for Information Systems Conference, Atlanta, GA, USA},
  volume={2324},
  number={36},
  year={2013}
}

@article{Arora2019,
abstract = {Understanding dependency relationship between various program elements in an object-oriented system is essential for many software engineering applications. In this paper, we propose a novel approach of transforming a Java project into a connected graph comprising of program elements (represented as graph nodes) connected to each other using ownership and dependency relationships (represented as edges). These graphs, named as JavaRelationshipGraphs (JRG) are created and stored using Neo4j Graph Database. Additionally, the proposed JavaRelationshipGraphs framework provides details about the two-staged conversion process along with the algorithms involved. The JRG framework uses compiled Java project to obtain the corresponding graph, which can be effectively visualized and queried using the Neo4j browser. JRG is capable of representing most of the important object-oriented features like inheritance, encapsulation, method overloading and overriding. Hence, they are suitable for use in software engineering applications like program dependence analysis, code mining, etc.},
author = {Arora, Ritu and Goel, Sanjay},
doi = {10.1145/3305160.3305173},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Arora, Goel - Transforming Java projects into graphs using neo4j graph databases - 2019.pdf:pdf},
isbn = {9781450366427},
journal = {ACM International Conference Proceeding Series},
keywords = {Dependency analysis in Java programs,Dependency graphs,Neo4j graph databases,Object-oriented languages},
pages = {80--84},
title = {{Transforming Java projects into graphs using neo4j graph databases}},
year = {2019}
}

@Misc{neo4j,
mendeley-groups = {Software Dependability/GithubMining},
title = {{Neo4j Graph Platform – The Leader in Graph Databases}},
url = {https://neo4j.com/},
howpublished = {https://github.com/neo4j/neo4j},
urldate = {2020-07-11}
}


@article{Ratzinger2007a,
abstract = {Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, propositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects. {\textcopyright} 2007 IEEE.},
author = {Ratzinger, Jacek and Sigmund, Thomas and Vorburger, Peter and Gall, Harald},
doi = {10.1109/ESEM.2007.63},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Ratzinger et al. - Mining software evolution to predict refactoring - 2007.pdf:pdf},
isbn = {0769528864},
journal = {Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007},
mendeley-groups = {Software Dependability/Related Work},
pages = {354--363},
title = {{Mining software evolution to predict refactoring}},
year = {2007}
}
@article{Valetto2007,
abstract = {We propose a quantitative measure of socio-technical congruence as an indicator of the performance of an organization in carrying out a software development project. We show how the information necessary to implement that measure can be mined from commonly used software repositories, and we describe how socio-technical congruence can be computed based on that information. {\textcopyright} 2007 IEEE.},
author = {Valetto, Giuseppe and Helander, Mary and Ehrlich, Kate and Chulani, Sunita and Wegman, Mark and Williams, Clay},
doi = {10.1109/MSR.2007.33},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Valetto et al. - Using software repositories to investigate socio-technical congruence in development projects - 2007.pdf:pdf},
isbn = {076952950X},
journal = {Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
pages = {12--15},
title = {{Using software repositories to investigate socio-technical congruence in development projects}},
year = {2007}
}
@article{Kwan2011b,
abstract = {Coordination in software engineering is necessary for software teams. To study coordination, researchers need a a way to conceptualize and measure it|one such measure is socio-technical congruence. Within a team setting, awareness of other's tasks and abilities enables coordination, but the conceptualizations for socio-technical congruence do not include awareness. In this paper, our goal is to include awareness in socio-technical congruence. To do this, we conduct an empirical investigation of a team's awareness behaviour. We examine how developers transmit awareness information in a global software-engineering environment in a project called Ship using direct observations, interviews, and a questionnaire. We found that team members were satisfied with using simple awareness mechanisms such as email and meetings. We also identified that experienced team members served as brokers and filled coordination gaps, and that team members used multiple types of media simultaneously. Based on this work, we propose an aggregated sociotechnical congruence measurement that can be used to specify multiple relationships, such as awareness relationships, as interactions that satisfy technical dependencies. {\textcopyright} 2011 ACM.},
author = {Kwan, Irwin and Damian, Daniela},
doi = {10.1145/2024645.2024652},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Kwan, Segal - Extending Socio-technical Congruence with Awareness Relationships - 2011.pdf:pdf},
isbn = {9781450308502},
journal = {SSE'11 - Proceedings of the 4th International Workshop on Social Software Engineering},
keywords = {awareness,collaboration,coordination,empirical software engineering,socio-technical congruence},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
pages = {23--30},
title = {{Extending socio-technical congruence with awareness relationships}},
year = {2011}
}
@article{Kwan2011a,
abstract = {Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination. {\textcopyright} 2011 IEEE Published by the IEEE Computer Society.},
author = {Kwan, Irwin and Schr{\"{o}}ter, Adrian and Damian, Daniela},
doi = {10.1109/TSE.2011.29},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Kwan, Schr{\"{o}}ter, Damian - Does socio-technical congruence have an effect on software build success A study of coordination in a software.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Awareness,Coordination,Empirical software engineering,Integration,Socio-technical congruence,Software quality},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
number = {3},
pages = {307--324},
title = {{Does socio-technical congruence have an effect on software build success? A study of coordination in a software project}},
volume = {37},
year = {2011}
}


@article{Herbsleb2007,
abstract = {Globally-distributed projects are rapidly becoming the norm for large software systems, even as it becomes clear that global distribution of a project seriously impairs critical coordination mechanisms. In this paper, I describe a desired future for global development and the problems that stand in the way of achieving that vision. I review research and lay out research challenges in four critical areas: software architecture, eliciting and communicating requirements, environments and tools, and orchestrating global development. I conclude by noting the need for a systematic understanding of what drives the need to coordinate and effective mechanisms for bringing it about. {\textcopyright} 2007 IEEE.},
author = {Herbsleb, James D.},
doi = {10.1109/FOSE.2007.11},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Herbsleb - Global software engineering The future of socio-technical coordination - 2007.pdf:pdf},
isbn = {0769528295},
journal = {FoSE 2007: Future of Software Engineering},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
pages = {188--198},
title = {{Global software engineering: The future of socio-technical coordination}},
year = {2007}
}
@article{Cataldo2006,
abstract = {Task dependencies drive the need to coordinate work activities. We describe a technique for using automatically generated archi-val data to compute coordination requirements, i.e., who must coordinate with whom to get the work done. Analysis of data from a large software development project revealed that coordina-tion requirements were highly volatile, and frequently extended beyond team boundaries. Congruence between coordination re-quirements and coordination activities shortened development time. Developers, particularly the most productive ones, changed their use of electronic communication media over time, achieving higher congruence. We discuss practical implications of our technique for the design of collaborative and awareness tools. {\textcopyright} Copyright 2006 ACM.},
author = {Cataldo, Marcelo and Wagstrom, Patrick A. and Herbsleb, James D. and Carley, Kathleen M.},
doi = {10.1145/1180875.1180929},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Cataldo et al. - Identification of coordination requirements Implications for the Design of collaboration and awareness tools - 2006.pdf:pdf},
isbn = {1595932496},
journal = {Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW},
keywords = {Collaboration tools,Coordination,Dynamic network analysis,Task awareness tools},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
pages = {353--362},
title = {{Identification of coordination requirements: Implications for the Design of collaboration and awareness tools}},
year = {2006}
}
@article{Kwan2011a,
abstract = {Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination. {\textcopyright} 2011 IEEE Published by the IEEE Computer Society.},
author = {Kwan, Irwin and Schr{\"{o}}ter, Adrian and Damian, Daniela},
doi = {10.1109/TSE.2011.29},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Kwan, Schr{\"{o}}ter, Damian - Does socio-technical congruence have an effect on software build success A study of coordination in a software.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Awareness,Coordination,Empirical software engineering,Integration,Socio-technical congruence,Software quality},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
number = {3},
pages = {307--324},
title = {{Does socio-technical congruence have an effect on software build success? A study of coordination in a software project}},
volume = {37},
year = {2011}
}
@article{Sarma2009,
abstract = {Software developers have long known that project success requires a robust understanding of both technical and social linkages. However, research has largely considered these independently. Research on networks of technical artifacts focuses on techniques like code analysis or mining project archives. Social network analysis has been used to capture information about relations among people. Yet, each type of information is often far more useful when combined, as when the "goodness" of social networks is judged by the patterns of dependencies in the technical artifacts. To bring such information together, we have developed Tesseract, an interactive exploratory environment that utilizes cross-linked displays to visualize the myriad relationships between artifacts, developers, bugs, and communications. We evaluated Tesseract by (1) demonstrating its feasibility with GNOME project data (2) assessing its usability via informal user evaluations, and (3) verifying its suitability for the open source community via semi-structured interviews. {\textcopyright} 2009 IEEE.},
author = {Sarma, Anita and Maccherone, Larry and Wagstrom, Patrick and Herbsleb, James},
doi = {10.1109/ICSE.2009.5070505},
file = {:Users/martinsteinhauer/Documents/Mendeley Desktop/Sarma et al. - Tesseract Interactive visual exploration of socio-technical relationships in software development - 2009.pdf:pdf},
isbn = {9781424434527},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
mendeley-groups = {Expose,Expose/Socio-Technical Congruence},
pages = {23--33},
publisher = {IEEE},
title = {{Tesseract: Interactive visual exploration of socio-technical relationships in software development}},
year = {2009}
}

@article{fontana2016comparing,
  title={Comparing and experimenting machine learning techniques for code smell detection},
  author={Fontana, Francesca Arcelli and M{\"a}ntyl{\"a}, Mika V and Zanoni, Marco and Marino, Alessandro},
  journal={Empirical Software Engineering},
  volume={21},
  number={3},
  pages={1143--1191},
  year={2016},
  publisher={Springer}
}

@article{fontana2012automatic,
  title={Automatic detection of bad smells in code: An experimental assessment.},
  author={Fontana, Francesca Arcelli and Braione, Pietro and Zanoni, Marco},
  journal={J. Object Technol.},
  volume={11},
  number={2},
  pages={5--1},
  year={2012}
}

@article{palomba2018large,
  title={A large-scale empirical study on the lifecycle of code smell co-occurrences},
  author={Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Fasano, Fausto and Oliveto, Rocco and De Lucia, Andrea},
  journal={Information and Software Technology},
  volume={99},
  pages={1--10},
  year={2018},
  publisher={Elsevier}
}
@inproceedings{di2018detecting,
  title={Detecting code smells using machine learning techniques: are we there yet?},
  author={Di Nucci, Dario and Palomba, Fabio and Tamburri, Damian A and Serebrenik, Alexander and De Lucia, Andrea},
  booktitle={2018 ieee 25th international conference on software analysis, evolution and reengineering (saner)},
  pages={612--621},
  year={2018},
  organization={IEEE}
}