
\section{Background and Limitations of the State of the Art}
\label{sec:background}
In this section, we provide background information on the frameworks available that ease the data extraction process of machine learning processes as well as their limitations.

\subsection{Related Work}
Previous research shows that there is a lack of reproducibility in most Git-based research projects \cite{Robles2010}. Since then, some new frameworks and libraries evolved targeting the traversal of Git projects.

Common used frameworks to analyze the commit history of software repositories are RepoDriller \cite{repodriller} and PyDriller \cite{pydriller,Spadini2018}. Whereas both tools simplify the traversing of Git commits, PyDriller is able to calculate some basic metrics like complexity and method count. The metric calculation is done within the library and could not easily be replaced with custom metric logic. Additionally, PyDriller states to be multithreaded and therefore improve the performance and lower the calculation time. A closer look into the library shows that this statement is only partially true: PyDriller is based on the library GitPython which is restricted by visiting only one commit at a time. This is caused by the underlying file-based Git archive that enables thread-safety by locking the files during e.g. check out operations. Hence it is only possible to parallelize the processing in the file dimension but not in the time dimension (the commit history). In other words, only the iteration over files within one commit and then checkout the next commit which can be parallelized. The same theory applied to RepoDriller which relies on the Java-based alternative JGit \cite{jgit}. And besides, both tools allow no multi-repository mining and require additional orchestration and process management.

Other projects try to avoid direct file system access and transferring repositories in an additional database before the first analysis. One well-known project is GHTorrent \cite{Gousios2012, Gousios2015} which allows various file-independent queries after importing the repository into a MySQL or MongoDB database. Unfortunately, the file content is not considered within this approach and only tooling for generating and importing is covered by them. 

The project `Public Git Archive' was introduced by researchers at \textsc{sourced} and enables the analysis of repositories from GitHub at a very large scale. They provide a lot of tools written in Go to query and also introduce a new storage format called Siva to reduce the stored size of forked repositories. Through a publicly available index file no additional processing is needed for developers. After the company was sold, the public available index file was taken down due to high costs and apparently was lost completely \cite{srcdissue}.

\subsection{Limitations of the State of the Art}
Despite the notable effort spent so far, in our investigation into the matter we noticed a number of limitations of the existing frameworks that can represent challenges to be addressed. 

\subsubsection{Reproducibility Challenge}
Related work shows that reproducibility of the mining pipeline is a key feature to enable other researchers an easy reproduction and verification of the used dataset. Therefore, hosted datasets can become a nightmare since they require additional (financial) resources of the project holder and also are dependent on the life cycle of the initial project. If the project gets deprecated, all data could be gone like in the case of sourced. Mining data directly from GitHub may not resolve the problem of deleted or switched to private repositories and also introduce some overhead in terms of cloning and preprocessing but decreases the dependence to a hosting company or the creator of the project. Consequently, we will focus on the raw Git archive as a source of data.

\subsubsection{Multi-Dimensionality of Git Data}
At first glance, a GitHub repository looks like a tree-like timeline which contains different revisions and file versions along the axes. But from an algorithmic perspective, that can rapidly become an issue in terms of performance as every axis results in an iteration loop to reach at least each file once. This also could be described as multi-dimensionality of the Git data, as each additional dimension added to the mining process, will result in more computation time and without considering the actual computation cost for detecting refactorings or calculating software quality metrics. Depending on the mining problem, we identified the following dimensions:

\begin{itemize}
	
    \item{\textbf{Time dimension:} Time dimension describes the total size of the analyzed commits. Git normally consists of one or more branches containing one main branch, denoted as \emph{master}. When traversing over the master branch, one will get all branches that have been merged into the master. In our case, we assume that for refactoring detection it would be sufficient to detect all valuable refactorings that have been considered as useful by merging them back into the master branch. Therefore, the time dimension of the refactoring detection could be seen as linear. When an analysis problem requires visiting every commit in the repository, the time dimension is considered as the total count of commits within the whole repository. }
    
    \smallskip
    \item{\textbf{File dimension:} Defined by the file count one commit has in total. This amount of files can vary significantly and often depend on the project size, age of the project or the used programming language. Additionally, it is important to note that the file dimension is different for each commit as a file can be added or deleted within a commit. If the latest commit has only a few files and all others have many, the file dimension will affect the performance more as if the last commit have many files and the others have only a few. }
    
    \smallskip
    \item{\textbf{Structural node dimension:} Similar to the file dimension, the total number of interesting nodes, for example, classes, methods or code lines, are counted in the content of each file. Considering object-oriented languages, a file normally contains one class, but also could consist of more than one class declaration. If the mining problem is dependent on method or class member declarations, those would also be considered as a number of interesting nodes per file.}
    
    \smallskip
    \item{\textbf{Metric dimension:} The last dimension is very specific particularly regarding the mining problem. In our example, we would like to extract refactoring operations and calculate software quality metrics. Therefore, the metric dimension is highly coupled with the structural node dimension and defined by the method the metric is collected. RefactoringMiner \cite{Tsantalis:ICSE:2018:RefactoringMiner}} uses an abstract UML algorithm to detect refactorings, while CK \cite{aniche-ck} is using an abstract syntax tree (AST). Those algorithms also add on top of the file or structural dimension, depending on how they are implemented and designed.
\end{itemize}

\subsubsection{Parallelization Challenge}
One of the biggest challenges is building a mining tool that allows scaling of the mining process along the presented dimensionality axes. Both presented tools, PyDriller and RepoDriller, do not or only partially support the parallelization. While RepoDriller does not do any kind of parallelization, PyDriller allows scaling along the file dimension axis. Files within a single commit can be analyzed by multiple threads and increase the performance and decrease the computation time as the use of those libraries shows \cite{Gote2019}. As the multi-dimensionality of GitHub repositories outlines, we can not presume that repositories contain many files. When a repository contains fewer files but has a large revision history, the runtime of the mining process will get worse.

As already explained, the structural node dimension and the metric dimension are highly dependent on the type of analysis that should be executed. To make this approach generalizable, we focus on parallelization along the time and file dimension axes. As seen in PyDriller, the file dimension can easily be parallelized. In opposite, the commit axis is rather difficult: Since Git writes lock files when accessing the repository to avoid file inconsistencies by modifying the repository, only one commit can be visited concurrently. This limitation reflects in JGit and GitPython, which has all file-based actions marked as non-threadsafe. This limitation results in libraries not implementing a time dimension parallelization approach.

\subsubsection{Generalization Challenge}
Even though we are conducting this distributed approach in the context of detecting refactoring operations, it is designed to be generalizable and can be reused for other types of mining problems. Since mining problems can vary from social aspects up to high technical questions like code evolution and smell detection, it is not easy to build a pipeline that matches all type of problems. Therefore, this pipeline concentrates on technical, content-based repository mining. Additionally, libraries are not standardized in their input and output design. While RefactoringMiner takes the repository and commits as an input, CK is not designed Git-specific and takes just the source directory as an input. This requires the pipeline to adopt the corresponding libraries in terms of inputs and outputs of each pipeline step and even more important to keep track of the memory usage, since the libraries itself write outputs to disk, while within a pipeline, results partially are stored in memory. This necessitates careful pipeline design and memory management to avoid out of memory exceptions.