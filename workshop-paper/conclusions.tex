
\section{Conclusion and Future Work}
\label{conclusion}
The presented prototype of the \iris framework clearly shows that parallelization of the time dimension can increase the performance in MSR. We also discussed the major issues that affecting the runtime negatively and analyzed the current problems of the implementation. Therefore, there is a lot of potential improving the prototype in terms of parallelization as well as generalization. The following section is presenting further steps to optimize the framework and make it more flexible for other mining tasks.\\
\textbf{Replacing the file-based repository by database.}
This step is needed to avoid problems introduced by file-based Git repositories and the described locking mechanism. This approach is inspired by the GHTorrent project \cite{Gousios2012, Gousios2015} and the Sourced project \cite{Markovtsev2018}. Unlike GHTorrent, we'd like to additionally store file contents together with meta-information about the project in a large database. To avoid hosting problems like in Sourced and improve reproducibility, our pipeline should support the automatic generation of this database by a given list of interesting repositories. It should be evaluated, which storage format is most suitable in interoperability with Spark (e.g. Apache Cassandra \cite{lakshman2010cassandra}, Neo4J \cite{neo4j} or just simple parquet files \cite{parquet}).\\
Using a database as input not only avoids the file lock in Git repositories, it is also likely to improve the overall performance since Spark can use query optimization and higher data distribution since it is not limited to file-based data sources. Also, basic operations like counting commits along a branch can be done much faster in a structured and pre-processed environment. The pre-processing overhead has to be evaluated.\\
\textbf{Generalization through standardized inputs and outputs.} Building a pipeline upon Apache Spark improves a standardized design of each analysis step. Building well-defined input and output formats for each processing step raise the level of reusability of certain processing steps and allow further research much easier adoption of other research questions.\\
\textbf{Graphs by default.} Spark enhances big data analysis by integrated support for graph data and adjusted, for distributed environments optimized graph algorithms \cite{graphx}. If the dataset provides access to graph-like structures makes it easier answering research-questions in the area of developer-interaction.\\
Additionally, it could be considered to store source code information not as plain text but as graph data using abstract syntax trees (AST). Smaller projects have already constructed such use cases in combination with Neo4j \cite{Arora2019}.\\
\textbf{Reproducibility by default.} Through pre-processing, the data becomes much easier to deal with and besides, there is no dependency on existing datasets because it can be generated by everyone just by accessing the public Git API. Furthermore, Spark allows us to run on common cloud platforms like Amazon AWS, Azure, or Google Cloud as well as on local clusters or even on a developer's laptop. This makes executing the pipeline fairly easy. 